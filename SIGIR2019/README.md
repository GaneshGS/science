# Requirements

Aside from a CSV of document identifiers and their OCR'd text, you will need to have the latest versions of Go and Python 3 installed. As well as the python packages for `sqlite3` and `zlib`.

# Running a Signature Method

## Preparations
Given an input CSV file, run the `splitcsv` tool to split it into multiple text documents ready for featurization.  Note that this tool will create output files in your *working directory* so be sure that it's the right place for the (perhaps many) source text documents.

## Featurizing documents
For each input text file `input.txt` run `mkgrams <input.txt >output.features` to generate the features for that file.

## Preprocessing hashes + generating random vectors.
For the `minhash`, `topsig`, and `topsig-weighted` methods, a central database containing the hashes, pre-generated random-vectors, global feature counts, and runtimes must be created.  To do so,
- Concatenate all the feature files from all of your input features into one global features file `global.features`.
- Run the `features2db.sh` script -- `features2db.sh global.features global.db` to generate a SQLite3 database `global.db` containing the central database.

## Methods
### `minhash`, `topsig`, `topsig-weighted`
These commands take in the central database along with the desired output hash size in bits.  They expect the featurized document on standard input.  These commands produce on standard output one line formatted as shown:

```<count of features in document> <hash value in base 10> <number of bits set in binary representation of hash> <elapsed time in nanoseconds>```.

To use:
```./<command> path-to-central-database.db hash-bits < input-feature-file.txt > output.txt```.

### `simhash`
`simhash` expects as command line arguments both the path to a file containing a newline-seperated list of featurized documents to process and the number of desired hash bits in the output. 

To use:
```./simhash list-of-files.txt hash-bits > concatenated-outputs.txt```.

`simhash` produces output similar to `minhash`, `topsig`, and `topsig-weighted`, except it generates one file with multiple lines, one for each document in `list-of-files.txt`, instead of one file with one line for each document.

## Collecting Statistics

### Pairwise similarity
We have provided a tool `pairwise` to analyze the pairwise similarity of hashes generated by the above signature methods.

To use: ```./pairwise list-of-files.txt d start-offset end-offset```
- `list-of-files.txt` -- list of output hash files produced by the `minhash`, `topsig`, `topsig-weighted`, and `simhash` methods.
- `d` -- Maximum Hamming distance to print similarity statistics for.
- `start-offset` -- Index of first hash to print statistics for.  Usually `0`, but can be otherwise if you plan to split the computation of pairwise similarity over multiple `pairwise` invocations.
- `end-offset` -- Index immediately _after_ the last hash to print statistics for.  Use `-1` to compute statistics for every hash after and including the hash at `start-offset`.

`pairwise` produces on standard output one line for every hash processed.  On that line, it prints the following:

```<hash> <#hashes at distance at most 0> <#hashes at distance at most 0> ... <#hashes at distance at most d>```.

# Getting EDGAR documents

 To begin retrieving EDGAR documents, you must first download a particular year's file lists using the SEC's public web frontend.
 ```bash
!/bin/bash
year=2005
for quarter in 1 2 3 4
do
  base="https://www.sec.gov/Archives/edgar/Oldloads/${year}/QTR${quarter}/"
  curl $base  | egrep -o '[0-9]+\.gz' > files
  sed -e "s_^_${base}_" files >> all.files
done
 ```

For each line in `all.files`, you can download the archive containing documents for a particular day in that quarter.

```bash
#!/bin/bash

for line in $(cat ${all.files})
do
  echo $line
  wget -P edgar-data $line
  sleep 15s # To play nicely with SEC servers
done
```

The resulting files are approximately XML files (with some added cruft), so you will likely need to do some custom parsing. For our purposes, we rendered only the HTML documents into PDF using `wkhtmltopdf` and then running those resulting documents through our internal OCR engine to replicate the user experience. We note that PDFs are present in the XML files and are Uuencoded. The Golang library `uuencode` by `sanylcs` is what we have found to work best. 